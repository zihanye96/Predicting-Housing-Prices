# Predicting Home Prices Using Advanced Regression Techniques

This repository contains the necessary code to reproduce my submission for a Kaggle competition called 
"House Prices: Advanced Regression Techniques", where I used generalized additive models, regression trees,
random forests, and neural networks in order to determine a model that accurately predicts home prices using the
provided information.

## Introduction
The dataset for this competition contains information on 1,460 residential homes in Ames, Iowa. The dataset includes 80 variables, such as the price that the home was sold for and characteristics of the house such as basement quality, overall square footage, and the exterior covering on the house. My goal for this project is to create a model that can predict the response variable (sale price) most accurately with a (relatively) small number of predictor variables. This means that the predictor variables used in this model won’t be redundant and will have low correlation with each other. I created the "best model" using manual feature selection, choosing only the variables that appeared to have a strong effect on sale price and modifying certain categorical variables so that their levels are more intuitive. That way, I can achieve a balance between accuracy and complexity.

That being said, since there are other modelling techniques that result in models that are difficult to interpret but are potentially more accurate (in this case, regression trees, random forests, and neural networks), I will include all the variables provided in the dataset so that I have all the information potentially need to make the most accurate prediction. So, for this project, I will create one “best interpretable model” and one “best difficult to interpret model".

## Methods

I will first consider four different models: the Generalized Additive Model (GAM), a regression tree, a random forest, and a neural network. The GAM will stay true to my criterion for the best model, so I will try to keep the GAM small and interpretable as possible while staying accurate.

An initial exploration of the dataset makes it clear that there are many NA’s in the dataset. These NA’s can signal important information for my models, so it is important that I properly address them before fitting a model.

The NA’s in many of the categorical variables indicated that what that variable was giving information about didn’t exist. For example, NA for Alley meant there was no Alley. So for these variables, I added a new category named “None” put all the observations with NA’s into that category for these variables. These variables are Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, and MiscFeature.

Similarly, for GarageYearBlt, NA meant there was no Garage, but since GarageYearBlt is a quantitative variable, I couldn’t add in a category, so I resorted to the single imputation method of replacing the NA’s with the mean of the column. 

Likewise, for Electrical, I added an “Other” category where I put all the observations with NA since NA didn’t necessarily mean that the house had no Electrical system. On the other hand, for LotFrontage, NA meant the house didn’t have a lot, so I changed all the NA’s in that column to 0. Both these approaches were different than how I treated MasVnrType and MasVnrArea, which had 8 observations that were NA. Since NA didn’t necessarily mean the house didn’t have any masonry veneer, and as 8 observations isn’t a significant amount in a dataset of 1460 observations, I decided to remove those rows and then further explore the dataset.

The first step in building the GAM is to reduce the 81 variables into a subset that contains only the most important ones for predicting sale price. To do that, I first created a heatmap with all of the quantitative variables in the dataset and looked at which ones were most correlated with sale price. I then looked at the correlation amongst the variables that I chose and eliminated some variables that were highly correlated with others in the chosen set. Again, the best model to me means being able to predict the response variable most accurately with a (relatively) small number of predictor variables. This means that the predictor variables in my “best model” won’t be redundant and will have low correlation with each other. The way I created the "best model" was through manual feature selection, choosing only the variables that appeared to have a strong effect on sale price. That way, I have a good balance between accuracy and complexity.

The variables that were most correlated with sale price (r>.8) are: GarageArea, GarageCars, Fireplaces, TotalRmsAbvGrd, FullBath, GrLivArea, 1stFlrSF, TotalBsmtSF, YearRemodAdd, YearBuilt, and OverallQual.

Looking at another heatmap with just these variables, I saw that Garage Area and GarageCars are correlated, so I removed GarageCars, which is a measurement of how many cars can fit in the garage. There are other pairs of variables, such as total number of rooms above ground and above ground living area, or total basement square footage and first floor square footage, that could potentially be correlated, but the relationships between these variables don’t seem as direct as garage area and the number of cars that fit in the garage, so I won’t remove any of these variables from my model.

After looking at the heatmaps, I examined the scatterplots between each quantitative predictor variable and sale price to check if I missed any non-linear relationships. I saw that BsmtFinSF1 looks like a non-linear variable, but considering how it’s one of two components for total basement square footage, which is already included in the model, I decided to exclude this variable from the model.

The quantitative predictor variables are: Garage Area, Fireplaces, Total Rooms Above Ground, Full Bath, Above Ground Living Area, 1st Floor Square Footage, Total Basement Square Footage, Remodel Date, Year Built, and Overall Quality. Next, I thought about whether or not there are interactions between these variables and agreed that I should include an interaction term between year remodelled and year built. If a house did not have any remodelling done, then it will have the construction date as its year remodelled. Therefore, I will create a spline for this interaction in my generalized additive model.

WI examined scatterplots of the relationships between each variable and sale price (using the plotExplore() function) and found that Garage Area, Year Built, and Overall Quality appear to have nonlinear relationships with sale price. Therefore, I used a generalized additive model to account for these nonlinear relationships.

Before I created the GAM, I also examined the 43 categorical variables in the dataset. I used plotExplore() to find the categorical variables that appear to have the strongest influence on price, leaving out the ones whose means are similar across all levels of that variable. I decided that Exterior1st, Exterior2nd, Exterior Quality, Basement Quality, Kitchen Quality, Neighborhood, and Saletype seem to have the largest effects. However, I saw that Exterior2nd is the exterior covering on the house if there’s more than one material. Upon further examination, I saw that 1245 out of 1460 of the observations had the same category for Exterior1st and Exterior2nd, and that the mean sale price of the houses with the same categories for both variables doesn’t differ significantly from the mean sale price of the houses with different categories between the two variables (180593 vs. 182820, although the sample sizes differ, it doesn’t seem like there is a significant difference still), so I won’t consider Exterior2nd, since it seems like a redundant variable.

Some of these categorical variables were problematic when cross-validating, as some levels did not have enough observations to do cross-validation with. Therefore, I decided to re-categorize the variables, grouping them into a smaller number of buckets. For example, with the neighborhood variable, I grouped the neighborhoods in four buckets: Low, MidLow, MidHigh, and High, with each level being representative of the average house price in that neighborhood. I did something similar with the SaleType variable, where I re-categorized the levels of SaleType into two buckets: low and high, based on the mean and the variability in sale price for each level of the original variable. Essentially, I wanted to group all of the levels of sale type that results in low sale prices together and all of the levels of sale type that results in high sale prices together. If a certain sale type results in a low sale price but also has high sale prices for some observations (ex. WD), I’ll put it in the high category just because that sale type has the potential for high sale prices as well. Here’s a more detailed explanation of all the re-categorizing I did for each categorical variable:

Exterior1st: Similar to neighborhood, except I split into 3 buckets: low, mid, high ExterQual: None of the
houses were in “poor” condition, so I grouped fair houses with average/typical houses and good houses with
excellent houses, since the distribution of sale prices for each new grouping seems similar.

BsmtQual: There are 37 observations that don’t have basements. For these, I created a new level called NoBasement. I combined the observations that had fair basement quality and no basements together into a level called fairno, since their boxplots look similar.

Ultimately, I fit a GAM that includes the following variables: Garage Area, Fireplaces, Total Rooms Above Ground, Full Bath, Above Ground Living Area, 1st Floor Square Footage, Total Basement Square Footage, Remodel Date, Year Built, Overall Quality, Neighborhood, Sale Type, Exterior 1st, Exterior Quality, and Basement Quality.

Next, I created a regression tree, random forest, and neural network, all of which were built using all the provided variables in the dataset. I allowed each model to choose which variables were valuable to include through its own respective criteria. I used the train() command in the caret package in R to fit these models as well. This was an automated process for the regression tree and the random forest, and for the neural network I used JMP to examine different combinations of hidden layers and nodes in each hidden layer to determine the configuration that results in the lowest RMSE. After converting the categorical variables to dummy variables and using JMP for model selection, I came upon two potential models: either I could use all the variables in a neural network with a single hidden layer of 7 nodes, or I could use the exact same variables I used for my linear regression and GAM models in a neural network with two hidden layers with 3 nodes each. Both models had approximately equal RMSE and there was no clear winner. Keeping interpretability as a tiebreaker, I used created the neural network with the same variables as my linear regression model and had two hidden layers with 3 nodes in each layer.

In the end, I chose the model that I submitted to Kaggle by looking at the prediction errors of each model using 5-fold cross validation. The random forest resulted in the least root mean squared error, so I chose that as the final model. However, the best model in my eyes is still the generalized additive model, as it’s far more interpretable than the random forest, and only marginally worse in prediction (the GAM’s RMSE is about $6000 higher).

## Results
After creating the models, I examined the 5-fold cross validation errors for each model to gauge their relative performance. Here are the root mean square errors I found using 5-fold cross validation:

GAM: 35983.39
Regression Tree: 43597.32
Random Forest: 29547.63
Neural Network: 50198.56
As stated earlier, I chose the model I submitted to Kaggle solely based off of the RMSE criterion. Therefore, the random forest that I created using all of the predictor variables was the final model. While the RMSE for the random forest is approximately 30,000 dollars, most of these houses sell for between 130,000 dollars and 214,000 dollars, so I wouldn’t say that the random forest’s predictions are horribly off. That being said, 30,000 dollars is a large sum of money and can have significant impact on the purchasing decision.


## Conclusion
From the models that I created, I was able to get a sense of which variables affect sale prices the most. Looking at the regression tree, it appears that the most important variables are overall quality, exterior quality, garage cars, above ground living area, and year built, all of which was included in the GAM. It makes sense that all these variables have significant effects on sale price, but through my visual exploration of the predictor variables, I was also able to determine that variables related to location, such as neighborhood, and other features such as basement condition and size and kitchen quality are important. 
After submitting my predictions to Kaggle, I got a root mean square logarithm error of .14528, which puts my model as rank #2173 upon submission on the leaderboard. There is a total of 4255 submissions, so that puts me at the 49th percentile. 
